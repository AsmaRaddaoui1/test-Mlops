"""
Pipeline modulaire pour la pr√©diction de churn client
Auteur: []
Date: []
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from scipy.stats import anderson, zscore

# Preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN

# Models
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier

# Metrics
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve, log_loss, cohen_kappa_score, 
    matthews_corrcoef
)

# Optimization
import optuna

def load_data(train_path, test_path):
    """
    Charge les donn√©es d'entra√Ænement et de test
    
    Parameters:
    train_path (str): Chemin vers le fichier d'entra√Ænement
    test_path (str): Chemin vers le fichier de test
    
    Returns:
    tuple: (X_train, y_train) - DataFrames contenant les donn√©es
    """
    try:
        X = pd.read_csv(train_path)
        y = pd.read_csv(test_path)
        print(f"‚úÖ Donn√©es charg√©es avec succ√®s")
        print(f"   - Train shape: {X.shape}")
        print(f"   - Test shape: {y.shape}")
        return X, y
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
        return None, None

def explore_data(X, y):
    """
    Explore les donn√©es avec des statistiques descriptives
    
    Parameters:
    X (DataFrame): Donn√©es d'entra√Ænement
    y (DataFrame): Donn√©es de test
    
    Returns:
    tuple: (X, y) - Les DataFrames inchang√©s
    """
    print("\nüîç Exploration des donn√©es:")
    print("=" * 50)
    
    # Informations de base
    print(f"Shape X: {X.shape}")
    print(f"Shape y: {y.shape}")
    
    # Types de donn√©es
    print("\nTypes de donn√©es (X):")
    print(X.dtypes.value_counts())
    
    # Valeurs manquantes
    print("\nValeurs manquantes (X):")
    print(X.isna().sum().sum())
    
    # Distribution de la target
    if 'Churn' in X.columns:
        print(f"\nDistribution de Churn (Train):")
        print(X['Churn'].value_counts())
    if 'Churn' in y.columns:
        print(f"\nDistribution de Churn (Test):")
        print(y['Churn'].value_counts())
    
    return X, y

def preprocess_data(X, y):
    """
    Pr√©traite les donn√©es (encodage, nettoyage)
    
    Parameters:
    X (DataFrame): Donn√©es d'entra√Ænement
    y (DataFrame): Donn√©es de test
    
    Returns:
    tuple: (X_processed, y_processed, encoders) - Donn√©es pr√©trait√©es et encodeurs
    """
    print("\nüîÑ Pr√©traitement des donn√©es...")
    
    # Cr√©er des copies pour √©viter les modifications originales
    X_processed = X.copy()
    y_processed = y.copy()
    
    # Conversion des types
    X_processed['Churn'] = X_processed['Churn'].astype(int)
    y_processed['Churn'] = y_processed['Churn'].astype(int)
    
    # Encodage des variables binaires
    X_processed['International plan'] = X_processed['International plan'].map({'No': 0, 'Yes': 1})
    y_processed['International plan'] = y_processed['International plan'].map({'No': 0, 'Yes': 1})
    
    X_processed['Voice mail plan'] = X_processed['Voice mail plan'].map({'No': 0, 'Yes': 1})
    y_processed['Voice mail plan'] = y_processed['Voice mail plan'].map({'No': 0, 'Yes': 1})
    
    # Encodage One-Hot pour State et Area code
    encoder_state = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    encoder_area = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    
    # Fit et transform sur les donn√©es d'entra√Ænement
    encoded_states_X = encoder_state.fit_transform(X_processed[['State']])
    encoded_area_X = encoder_area.fit_transform(X_processed[['Area code']])
    
    # Transform sur les donn√©es de test
    encoded_states_y = encoder_state.transform(y_processed[['State']])
    encoded_area_y = encoder_area.transform(y_processed[['Area code']])
    
    # Cr√©ation des DataFrames encod√©s
    encoded_states_df_X = pd.DataFrame(
        encoded_states_X, 
        columns=encoder_state.get_feature_names_out(['State'])
    )
    encoded_states_df_y = pd.DataFrame(
        encoded_states_y, 
        columns=encoder_state.get_feature_names_out(['State'])
    )
    
    encoded_area_df_X = pd.DataFrame(
        encoded_area_X, 
        columns=encoder_area.get_feature_names_out(['Area code'])
    )
    encoded_area_df_y = pd.DataFrame(
        encoded_area_y, 
        columns=encoder_area.get_feature_names_out(['Area code'])
    )
    
    # Suppression des colonnes originales et concat√©nation
    X_processed = X_processed.drop(['State', 'Area code'], axis=1)
    y_processed = y_processed.drop(['State', 'Area code'], axis=1)
    
    X_processed = pd.concat([X_processed, encoded_states_df_X, encoded_area_df_X], axis=1)
    y_processed = pd.concat([y_processed, encoded_states_df_y, encoded_area_df_y], axis=1)
    
    encoders = {
        'state_encoder': encoder_state,
        'area_encoder': encoder_area
    }
    
    print(f"‚úÖ Pr√©traitement termin√©")
    print(f"   - X shape apr√®s pr√©traitement: {X_processed.shape}")
    print(f"   - y shape apr√®s pr√©traitement: {y_processed.shape}")
    
    return X_processed, y_processed, encoders

def handle_outliers(X):
    """
    G√®re les outliers selon le type de distribution
    
    Parameters:
    X (DataFrame): Donn√©es √† traiter
    
    Returns:
    DataFrame: Donn√©es sans outliers
    """
    print("\nüìä Gestion des outliers...")
    
    selected_columns = [
        'Account length', 'Total day minutes', 'Total day calls', 'Total day charge',
        'Total eve minutes', 'Total eve calls', 'Total eve charge',
        'Total night minutes', 'Total night calls', 'Total night charge',
        'Total intl minutes', 'Total intl calls', 'Total intl charge'
    ]
    
    # V√©rifier quelles colonnes existent dans X
    available_columns = [col for col in selected_columns if col in X.columns]
    
    # S√©paration bas√©e sur le test Anderson-Darling
    selected_normal_columns = []
    selected_other_columns = []
    
    for column in available_columns:
        try:
            result = anderson(X[column])
            if result.statistic < result.critical_values[2]:
                selected_normal_columns.append(column)
            else:
                selected_other_columns.append(column)
        except:
            selected_other_columns.append(column)
    
    print(f"   - Colonnes normales: {len(selected_normal_columns)}")
    print(f"   - Colonnes non-normales: {len(selected_other_columns)}")
    
    # M√©thode Z-Score pour les distributions normales
    if selected_normal_columns:
        z_scores = zscore(X[selected_normal_columns])
        abs_z_scores = np.abs(z_scores)
        filtered_entries = (abs_z_scores < 3).all(axis=1)
        X = X[filtered_entries]
        print(f"   - Outliers Z-score supprim√©s: {len(X)} √©chantillons restants")
    
    # M√©thode IQR pour les autres distributions
    def remove_outliers_iqr(df, columns):
        for column in columns:
            Q1 = df[column].quantile(0.25)
            Q3 = df[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
        return df
    
    if selected_other_columns:
        X = remove_outliers_iqr(X, selected_other_columns)
        print(f"   - Outliers IQR supprim√©s: {len(X)} √©chantillons restants")
    
    return X

def create_features(X, y):
    """
    Cr√©e de nouvelles features
    
    Parameters:
    X (DataFrame): Donn√©es d'entra√Ænement
    y (DataFrame): Donn√©es de test
    
    Returns:
    tuple: (X, y) - DataFrames avec nouvelles features
    """
    print("\nüéØ Cr√©ation de nouvelles features...")
    
    # Features pour X
    X['Total calls'] = (X['Total day calls'] + X['Total eve calls'] + 
                       X['Total night calls'] + X['Total intl calls'])
    X['Total charge'] = (X['Total day charge'] + X['Total eve charge'] + 
                        X['Total night charge'] + X['Total intl charge'])
    X['CScalls Rate'] = X['Customer service calls'] / X['Account length']
    
    # Features pour y
    y['Total calls'] = (y['Total day calls'] + y['Total eve calls'] + 
                       y['Total night calls'] + y['Total intl calls'])
    y['Total charge'] = (y['Total day charge'] + y['Total eve charge'] + 
                        y['Total night charge'] + y['Total intl charge'])
    y['CScalls Rate'] = y['Customer service calls'] / y['Account length']
    
    print("‚úÖ Nouvelles features cr√©√©es: Total calls, Total charge, CScalls Rate")
    
    return X, y

def remove_correlated_features(X, y):
    """
    Supprime les features corr√©l√©es
    
    Parameters:
    X (DataFrame): Donn√©es d'entra√Ænement
    y (DataFrame): Donn√©es de test
    
    Returns:
    tuple: (X, y) - DataFrames sans features corr√©l√©es
    """
    print("\nüîç Suppression des features corr√©l√©es...")
    
    correlated_columns = [
        'Total day minutes', 'Total eve minutes', 'Total night minutes', 
        'Total intl minutes', 'Voice mail plan'
    ]
    
    # Ne supprimer que les colonnes qui existent
    columns_to_drop = [col for col in correlated_columns if col in X.columns]
    
    X = X.drop(columns_to_drop, axis=1)
    y = y.drop(columns_to_drop, axis=1)
    
    print(f"‚úÖ Features supprim√©es: {columns_to_drop}")
    
    return X, y

def prepare_training_data(X, y, sampling_strategy=0.3):
    """
    Pr√©pare les donn√©es pour l'entra√Ænement
    
    Parameters:
    X (DataFrame): Donn√©es d'entra√Ænement
    y (DataFrame): Donn√©es de test
    sampling_strategy (float): Ratio pour la r√©√©chantillonnage
    
    Returns:
    tuple: Donn√©es pr√©par√©es pour l'entra√Ænement
    """
    print("\nüìö Pr√©paration des donn√©es pour l'entra√Ænement...")
    
    # S√©paration features/target
    X_train = X.drop(['Churn'], axis=1)
    y_train = X['Churn']
    X_test = y.drop(['Churn'], axis=1)
    y_test = y['Churn']
    
    print(f"   - X_train shape: {X_train.shape}")
    print(f"   - y_train shape: {y_train.shape}")
    print(f"   - Distribution initiale: {y_train.value_counts().to_dict()}")
    
    # R√©√©chantillonnage pour g√©rer le d√©s√©quilibre
    smote_enn = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)
    X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)
    
    print(f"   - Apr√®s r√©√©chantillonnage: {pd.Series(y_resampled).value_counts().to_dict()}")
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_resampled)
    X_test_scaled = scaler.transform(X_test)
    
    print("‚úÖ Donn√©es pr√©par√©es avec succ√®s")
    
    return X_train_scaled, X_test_scaled, y_resampled, y_test, scaler

def initialize_models():
    """
    Initialise tous les mod√®les √† tester
    
    Returns:
    dict: Dictionnaire des mod√®les initialis√©s
    """
    models = {
        "Logistic Regression": LogisticRegression(class_weight='balanced', random_state=42),
        "Support Vector Machine": SVC(class_weight='balanced', random_state=42, probability=True),
        "Decision Tree": DecisionTreeClassifier(class_weight='balanced', random_state=42),
        "Random Forest": RandomForestClassifier(class_weight='balanced', random_state=42),
        "Naive Bayes": GaussianNB(),
        "K-Nearest Neighbors": KNeighborsClassifier(),
        "Gradient Boosting": GradientBoostingClassifier(random_state=42),
        "AdaBoost": AdaBoostClassifier(random_state=42),
        "XGBoost": XGBClassifier(random_state=42),
        "Neural Network": MLPClassifier(random_state=42, max_iter=1000)
    }
    
    return models

def train_and_evaluate_models(models, X_train, X_test, y_train, y_test):
    """
    Entra√Æne et √©value tous les mod√®les
    
    Parameters:
    models (dict): Dictionnaire des mod√®les
    X_train (array): Features d'entra√Ænement
    X_test (array): Features de test
    y_train (array): Target d'entra√Ænement
    y_test (array): Target de test
    
    Returns:
    dict: R√©sultats de l'√©valuation
    """
    print("\nü§ñ Entra√Ænement et √©valuation des mod√®les...")
    print("=" * 60)
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\nüîß Entra√Ænement de {model_name}...")
        
        try:
            # Entra√Ænement
            model.fit(X_train, y_train)
            
            # Pr√©dictions
            y_pred = model.predict(X_test)
            
            # √âvaluation
            accuracy = accuracy_score(y_test, y_pred)
            cm = confusion_matrix(y_test, y_pred)
            cr = classification_report(y_test, y_pred)
            
            # M√©triques suppl√©mentaires si disponibles
            metrics_dict = {
                'model': model,
                'accuracy': accuracy,
                'confusion_matrix': cm,
                'classification_report': cr
            }
            
            # Probabilities pour les m√©triques avanc√©es
            if hasattr(model, 'predict_proba'):
                y_proba = model.predict_proba(X_test)[:, 1]
                metrics_dict['roc_auc'] = roc_auc_score(y_test, y_proba)
                metrics_dict['log_loss'] = log_loss(y_test, y_proba)
            
            results[model_name] = metrics_dict
            
            print(f"   ‚úÖ {model_name} - Accuracy: {accuracy:.4f}")
            
        except Exception as e:
            print(f"   ‚ùå Erreur avec {model_name}: {e}")
            results[model_name] = {'error': str(e)}
    
    return results

def optimize_random_forest(X_train, X_test, y_train, y_test, n_trials=50):
    """
    Optimise Random Forest avec Optuna
    
    Parameters:
    X_train (array): Features d'entra√Ænement
    X_test (array): Features de test
    y_train (array): Target d'entra√Ænement
    y_test (array): Target de test
    n_trials (int): Nombre d'essais d'optimisation
    
    Returns:
    dict: Meilleurs hyperparam√®tres
    """
    print(f"\nüéØ Optimisation de Random Forest ({n_trials} essais)...")
    
    def objective(trial):
        n_estimators = trial.suggest_int('n_estimators', 100, 500, step=50)
        max_depth = trial.suggest_int('max_depth', 5, 30, step=5)
        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)
        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])
        
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            random_state=42
        )
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        return accuracy_score(y_test, y_pred)
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)
    
    print(f"‚úÖ Meilleurs param√®tres RF: {study.best_params}")
    print(f"‚úÖ Meilleure accuracy: {study.best_value:.4f}")
    
    return study.best_params

def optimize_xgboost(X_train, X_test, y_train, y_test, n_trials=50):
    """
    Optimise XGBoost avec Optuna
    
    Parameters:
    X_train (array): Features d'entra√Ænement
    X_test (array): Features de test
    y_train (array): Target d'entra√Ænement
    y_test (array): Target de test
    n_trials (int): Nombre d'essais d'optimisation
    
    Returns:
    dict: Meilleurs hyperparam√®tres
    """
    print(f"\nüéØ Optimisation de XGBoost ({n_trials} essais)...")
    
    def objective(trial):
        n_estimators = trial.suggest_int('n_estimators', 50, 300, step=50)
        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)
        max_depth = trial.suggest_int('max_depth', 3, 10)
        min_child_weight = trial.suggest_int('min_child_weight', 1, 10)
        subsample = trial.suggest_float('subsample', 0.5, 1.0)
        colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
        gamma = trial.suggest_float('gamma', 0, 5)
        
        model = XGBClassifier(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            min_child_weight=min_child_weight,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            gamma=gamma,
            random_state=42
        )
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        return accuracy_score(y_test, y_pred)
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)
    
    print(f"‚úÖ Meilleurs param√®tres XGBoost: {study.best_params}")
    print(f"‚úÖ Meilleure accuracy: {study.best_value:.4f}")
    
    return study.best_params

def evaluate_model(model, X_test, y_test, model_name=""):
    """
    √âvalue un mod√®le avec plusieurs m√©triques
    
    Parameters:
    model: Mod√®le entra√Æn√©
    X_test (array): Features de test
    y_test (array): Target de test
    model_name (str): Nom du mod√®le pour l'affichage
    
    Returns:
    dict: M√©triques d'√©valuation
    """
    print(f"\nüìä √âvaluation d√©taill√©e de {model_name}")
    print("=" * 50)
    
    y_pred = model.predict(X_test)
    
    # M√©triques de base
    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    cr = classification_report(y_test, y_pred)
    
    # M√©triques avanc√©es
    kappa = cohen_kappa_score(y_test, y_pred)
    mcc = matthews_corrcoef(y_test, y_pred)
    
    metrics_dict = {
        'accuracy': accuracy,
        'kappa': kappa,
        'mcc': mcc,
        'confusion_matrix': cm,
        'classification_report': cr
    }
    
    # M√©triques avec probabilities
    if hasattr(model, 'predict_proba'):
        y_proba = model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_proba)
        logloss = log_loss(y_test, y_proba)
        metrics_dict.update({
            'roc_auc': roc_auc,
            'log_loss': logloss
        })
    
    # Affichage des r√©sultats
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Cohen's Kappa: {kappa:.4f}")
    print(f"MCC: {mcc:.4f}")
    
    if 'roc_auc' in metrics_dict:
        print(f"ROC AUC: {metrics_dict['roc_auc']:.4f}")
        print(f"Log Loss: {metrics_dict['log_loss']:.4f}")
    
    print(f"\nMatrice de confusion:")
    print(cm)
    print(f"\nRapport de classification:")
    print(cr)
    
    return metrics_dict

def plot_roc_curve(model, X_test, y_test, model_name=""):
    """
    Trace la courbe ROC
    
    Parameters:
    model: Mod√®le entra√Æn√©
    X_test (array): Features de test
    y_test (array): Target de test
    model_name (str): Nom du mod√®le
    """
    if hasattr(model, 'predict_proba'):
        y_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_proba)
        roc_auc = roc_auc_score(y_test, y_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (area = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {model_name}')
        plt.legend(loc='lower right')
        plt.grid(True)
        plt.show()

# ...existing code...
def save_model(model, scaler, encoders, filepath):
    """
    Sauvegarde le mod√®le, le scaler et les encodeurs dans un seul fichier joblib/pka.
    Accepte les extensions: .joblib, .pkl, .pka (pr√©f√©rer .pka si demand√©).
    """
    try:
        import os
        parent = os.path.dirname(filepath)
        if parent and not os.path.exists(parent):
            os.makedirs(parent, exist_ok=True)

        # Autoriser .pka comme extension valide
        if not filepath.lower().endswith(('.joblib', '.pkl', '.pka')):
            filepath = filepath + '.joblib'

        bundle = {
            "model": model,
            "scaler": scaler,
            "encoders": encoders,
            "metadata": {
                "saved_at": pd.Timestamp.now(),
                "model_type": type(model).__name__
            }
        }

        # compression raisonnable
        joblib.dump(bundle, filepath, compress=3)
        print(f"‚úÖ Mod√®le sauvegard√© avec succ√®s dans : {filepath}")
        return filepath
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde du mod√®le : {e}")
        return None

def load_saved_model(filepath):
    """
    Recharge le mod√®le + scaler + encodeurs depuis un fichier joblib/pkl/pka.
    Retourne tuple (model, scaler, encoders) ou (None, None, None) en cas d'erreur.
    """
    try:
        import os
        candidates = [filepath]

        # si le chemin fourni n'existe pas, tenter avec extensions usuelles
        if not os.path.exists(filepath):
            for ext in ('.pka', '.joblib', '.pkl'):
                if os.path.exists(filepath + ext):
                    candidates = [filepath + ext]
                    break

        bundle = None
        tried = []
        for f in candidates:
            tried.append(f)
            try:
                bundle = joblib.load(f)
                filepath = f
                break
            except Exception:
                bundle = None

        if bundle is None:
            # essayer toutes les extensions si les candidats initiaux n'ont pas march√©
            for ext in ('.pka', '.joblib', '.pkl'):
                path_with_ext = filepath if filepath.lower().endswith(ext) else filepath + ext
                if os.path.exists(path_with_ext):
                    try:
                        bundle = joblib.load(path_with_ext)
                        filepath = path_with_ext
                        break
                    except Exception:
                        bundle = None

        if bundle is None:
            print(f"‚ùå Impossible de charger le fichier. Fichiers test√©s: {tried}")
            return None, None, None

        print(f"‚úÖ Mod√®le charg√© avec succ√®s depuis : {filepath}")
        return bundle.get("model"), bundle.get("scaler"), bundle.get("encoders")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement : {e}")
        return None, None, None
# ...existing code...

def load_model(filepath):
    """
    Charge le mod√®le sauvegard√© (alias pour load_saved_model pour la compatibilit√©)
    
    Parameters:
    filepath (str): Chemin vers le fichier du mod√®le
    
    Returns:
    dict: Dictionnaire contenant le mod√®le, scaler et encodeurs
    """
    model, scaler, encoders = load_saved_model(filepath)
    if model is not None:
        return {
            'model': model,
            'scaler': scaler,
            'encoders': encoders
        }
    return None